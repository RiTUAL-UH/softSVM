{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Model . . . \n",
      "google w2v loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"loading Model . . . \")\n",
    "model =   KeyedVectors.load_word2vec_format('../../../embedding/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(\"google w2v loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO w2v loaded\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embeddings_index = {}\n",
    "SO_file = 'textModel'\n",
    "model2 = Word2Vec.load('../../../embedding/'+ SO_file)\n",
    "print(\"SO w2v loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pk\n",
    "preprocessed_data = \"preprocessed_data_noClue_T+B_medium/\"   \n",
    "train_q1_Body= pk.load(open(preprocessed_data+\"train_q1_Body.pkl\", 'rb'))\n",
    "train_q2_Body= pk.load(open(preprocessed_data+\"train_q2_Body.pkl\", 'rb'))\n",
    "train_labels= pk.load(open(preprocessed_data+\"train_class.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "with open(preprocessed_data+\"test_q1_Body.pkl\", 'rb') as fb:\n",
    "    test_q1_Body= pk.load(fb)\n",
    "\n",
    "with open(preprocessed_data+\"test_q2_Body.pkl\", 'rb') as fb:\n",
    "    test_q2_Body= pk.load(fb)\n",
    "    \n",
    "with open(preprocessed_data+\"test_class.pkl\", 'rb') as fb:\n",
    "    test_labels= pk.load(fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_lst =  train_q1_Body + train_q2_Body +test_q1_Body  + test_q2_Body "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL fitted!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize,\n",
    "                                             analyzer=\"word\", lowercase=True)\n",
    "fittedTFIDFvectorizer = tfidf_vectorizer.fit_transform(all_text_lst)\n",
    "print(\"MODEL fitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def common_bigrams( q1, q2, model):\n",
    "    q1_words = nltk.word_tokenize(q1)\n",
    "    q1BiGrams = list(nltk.bigrams(q1_words))\n",
    "    q2_words = nltk.word_tokenize(q2)\n",
    "    q2BiGrams = list(nltk.bigrams(q2_words))\n",
    "\n",
    "    matches = 0\n",
    "    for questionBigram in q1BiGrams:\n",
    "        for parentBigram in q2BiGrams:\n",
    "            if questionBigram == parentBigram:\n",
    "                matches += 1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_trigrams( q1, q2, model):\n",
    "    q1_words = nltk.word_tokenize(q1)\n",
    "    q1BiGrams = list(nltk.trigrams(q1_words))\n",
    "    q2_words = nltk.word_tokenize(q2)\n",
    "    q2BiGrams = list(nltk.trigrams(q2_words))\n",
    "\n",
    "    matches = 0\n",
    "    for questionBigram in q1BiGrams:\n",
    "        for parentBigram in q2BiGrams:\n",
    "            if questionBigram == parentBigram:\n",
    "                matches += 1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_char_trigrams( q1, q2, model):\n",
    "    char_q1 = [q1[i:i+3] for i in range(len(q1)-1)]\n",
    "    char_q2 = [q2[i:i+3] for i in range(len(q2)-1)]\n",
    "    matches = 0\n",
    "    for c1 in char_q1:\n",
    "        for c2 in char_q2:\n",
    "            if c1==c2:\n",
    "                matches+=1\n",
    "    return matches\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_char_fourgrams( q1, q2, model):\n",
    "    char_q1 = [q1[i:i+4] for i in range(len(q1)-1)]\n",
    "    char_q2 = [q2[i:i+4] for i in range(len(q2)-1)]\n",
    "    matches = 0\n",
    "    for c1 in char_q1:\n",
    "        for c2 in char_q2:\n",
    "            if c1==c2:\n",
    "                matches+=1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_char_fivegrams( q1, q2, model):\n",
    "    char_q1 = [q1[i:i+5] for i in range(len(q1)-1)]\n",
    "    char_q2 = [q2[i:i+5] for i in range(len(q2)-1)]\n",
    "    matches = 0\n",
    "    for c1 in char_q1:\n",
    "        for c2 in char_q2:\n",
    "            if c1==c2:\n",
    "                matches+=1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_unigrams( q1, q2, model):\n",
    "    q1_words =nltk.word_tokenize(q1)\n",
    "    q2_words = nltk.word_tokenize(q2)\n",
    "    matches = 0\n",
    "    for q1 in q1_words:\n",
    "        for q2 in q2_words:\n",
    "            if q2 == q1:\n",
    "                matches += 1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(common_unigrams(\"hello \",\"hello\", model ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def soft_w2v(q1, q2, model,fittedTFIDF):\n",
    "    if len(q1)==0 or len(q2)==0: return 0\n",
    "    matrix0, matrix1 ,featureNames = vectorizer(q1 , q2, fittedTFIDF)\n",
    "    index = []\n",
    "    for i in np.nonzero(matrix0)[0]:\n",
    "        index.append(i)\n",
    "    for i in np.nonzero(matrix1)[0]:\n",
    "        index.append(i)\n",
    "    index = sorted(set(index))\n",
    "    new_matrix0 = []\n",
    "    new_matrix1 = []\n",
    "    for l in index:\n",
    "        new_matrix0.append(matrix0[l])\n",
    "        new_matrix1.append(matrix1[l])\n",
    "    docs = []\n",
    "    docs.append(q1)\n",
    "    docs.append(q2)\n",
    "    featureNameLSZT = []\n",
    "    for g in index:\n",
    "        featureNameLSZT.append(featureNames[g])\n",
    "    matrix = matrixByw2v(featureNameLSZT, model)\n",
    "    npM0 = np.asarray(new_matrix0)\n",
    "    npM1 = np.asarray(new_matrix1)\n",
    "    tnpM0 = npM0.transpose()\n",
    "    tnpM1 = npM1.transpose()\n",
    "    dotProduct1 = np.dot(tnpM0, matrix)\n",
    "    dotproduct2 = np.dot(dotProduct1, npM1)\n",
    "    firstDomPartOne = np.dot(tnpM0, matrix)\n",
    "    firstDomPartTwo = np.dot(firstDomPartOne, npM0)\n",
    "    firstDominator = math.sqrt(firstDomPartTwo)\n",
    "    secondDomPartOne = np.dot(tnpM1, matrix)\n",
    "    secondDomPartTwo = np.dot(secondDomPartOne, npM1)\n",
    "    secondDominator = math.sqrt(secondDomPartTwo)\n",
    "    softCosineSimilarity = dotproduct2 / (firstDominator * secondDominator)\n",
    "    if np.count_nonzero(new_matrix0)==0 or np.count_nonzero(new_matrix1)==0:\n",
    "        return 0\n",
    "    return softCosineSimilarity\n",
    "\n",
    "def matrixByw2v(featureNames, model):\n",
    "\n",
    "    matrix = np.zeros((len(featureNames), len(featureNames))).astype('float')\n",
    "    # matrix =[[]]\n",
    "    for i in range(len(featureNames)):\n",
    "        for j in range(len(featureNames)):\n",
    "            if i == j:\n",
    "                matrix[i][j] = 1\n",
    "            else:\n",
    "                try:\n",
    "                    sim = model.similarity(featureNames[i], featureNames[j])\n",
    "                except KeyError:\n",
    "                    sim = 0\n",
    "                matrix[i][j] = (max(0, sim) * max(0, sim))\n",
    "    return matrix\n",
    "\n",
    "def vectorizer(question, parentQuestion, fittedTFIDF):\n",
    "    if len(question) == 0 or len(parentQuestion) == 0:\n",
    "        return [0]\n",
    "    documents = []\n",
    "    documents.append(question)\n",
    "    documents.append(parentQuestion)\n",
    "    tfidf_matrix = fittedTFIDF.transform(documents).toarray()\n",
    "    featureNames = fittedTFIDF.get_feature_names()\n",
    "    return tfidf_matrix[0], tfidf_matrix[1] ,featureNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def soft_lev(q1, q2, model,fittedTFIDF):\n",
    "    if len(q1)==0 or len(q2)==0: return 0\n",
    "    matrix0, matrix1 ,featureNames = vectorizer(q1 , q2, fittedTFIDF)\n",
    "    index = []\n",
    "    for i in np.nonzero(matrix0)[0]:\n",
    "        index.append(i)\n",
    "    for i in np.nonzero(matrix1)[0]:\n",
    "        index.append(i)\n",
    "    index = sorted(set(index))\n",
    "    new_matrix0 = []\n",
    "    new_matrix1 = []\n",
    "    for l in index:\n",
    "        new_matrix0.append(matrix0[l])\n",
    "        new_matrix1.append(matrix1[l])\n",
    "    docs = []\n",
    "    docs.append(q1)\n",
    "    docs.append(q2)\n",
    "    featureNameLSZT = []\n",
    "    for g in index:\n",
    "        featureNameLSZT.append(featureNames[g])\n",
    "    matrix = matrixByLev(featureNameLSZT, model)\n",
    "    npM0 = np.asarray(new_matrix0)\n",
    "    npM1 = np.asarray(new_matrix1)\n",
    "    tnpM0 = npM0.transpose()\n",
    "    tnpM1 = npM1.transpose()\n",
    "    dotProduct1 = np.dot(tnpM0, matrix)\n",
    "    dotproduct2 = np.dot(dotProduct1, npM1)\n",
    "    firstDomPartOne = np.dot(tnpM0, matrix)\n",
    "    firstDomPartTwo = np.dot(firstDomPartOne, npM0)\n",
    "    firstDominator = math.sqrt(firstDomPartTwo)\n",
    "    secondDomPartOne = np.dot(tnpM1, matrix)\n",
    "    secondDomPartTwo = np.dot(secondDomPartOne, npM1)\n",
    "    secondDominator = math.sqrt(secondDomPartTwo)\n",
    "    softCosineSimilarity = dotproduct2 / (firstDominator * secondDominator)\n",
    "    if np.count_nonzero(new_matrix0)==0 or np.count_nonzero(new_matrix1)==0:\n",
    "        return 0\n",
    "    return softCosineSimilarity\n",
    "\n",
    "def matrixByLev(featureNames, model):\n",
    "\n",
    "    matrix = np.zeros((len(featureNames), len(featureNames))).astype('float')\n",
    "    # matrix =[[]]\n",
    "    for i in range(len(featureNames)):\n",
    "        for j in range(len(featureNames)):\n",
    "\n",
    "            if i == j:\n",
    "                matrix[i][j] = 1\n",
    "            else:\n",
    "                # print(\"similarity of word {0} and {1}\".format(featureNames[i], featureNames[j]))\n",
    "                try:\n",
    "                    lev = Levenshtein.distance(featureNames[i], featureNames[j])\n",
    "                    lenfirst = len(featureNames[i])\n",
    "                    lensecond = len(featureNames[j])\n",
    "                    matrix[i][j] = 1.8 * (math.pow(1 - lev / max(lenfirst, lensecond), 5))\n",
    "                except KeyError:\n",
    "                    print(\"error in edit distance\")\n",
    "\n",
    "\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cos_similarity(q1, q2, model,fittedTFIDF):\n",
    "    \n",
    "    \n",
    "    if len(q1)==0 or len(q2) ==0:\n",
    "        return 0\n",
    "    documents =[]\n",
    "    documents.append(q1)\n",
    "    documents.append(q2)\n",
    "    tfidf_matrix = fittedTFIDF.transform(documents)\n",
    "    matrix = cosine_similarity(tfidf_matrix[0,:], tfidf_matrix[1,:])\n",
    "    return float(matrix[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(cos_similarity(\"hi hello hell\",\"hello hell hi\",model,tfidf_vectorizer ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_bigrams =[]\n",
    "train_unigrams =[]\n",
    "train_trigrams =[]\n",
    "train_char_three = []\n",
    "train_char_four =[]\n",
    "train_char_five =[]\n",
    "soft_train_google =[]\n",
    "soft_train_SO =[]\n",
    "soft_train_lev = []\n",
    "\n",
    "cos_train_body =[]\n",
    "\n",
    "test_bigrams =[]\n",
    "test_unigrams =[]\n",
    "test_trigrams =[]\n",
    "test_char_three = []\n",
    "test_char_four =[]\n",
    "test_char_five =[]\n",
    "soft_test_google =[]\n",
    "soft_test_SO =[]\n",
    "soft_test_lev = []\n",
    "\n",
    "cos_test_body =[]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/anaconda3-5.2.0/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/share/apps/anaconda3-5.2.0/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "print(len(train_q1_Body))\n",
    "for i in range(len(train_q1_Body)):\n",
    "    \n",
    "        postText = train_q1_Body[i] \n",
    "        relatedPostText =  train_q2_Body[i] \n",
    "\n",
    "        train_char_five.append(common_char_fivegrams(postText, relatedPostText, model))\n",
    "        train_char_four.append(common_char_fourgrams(postText, relatedPostText, model))\n",
    "        train_char_three.append(common_char_trigrams(postText, relatedPostText, model))\n",
    "        train_trigrams.append(common_trigrams(postText, relatedPostText, model))\n",
    "\n",
    "        train_bigrams.append(common_bigrams(postText, relatedPostText, model))\n",
    "        train_unigrams.append(common_unigrams(postText, relatedPostText, model))\n",
    "\n",
    "        softw2v1 =soft_w2v(postText, relatedPostText, model, tfidf_vectorizer)\n",
    "\n",
    "        soft_train_google.append(softw2v1)\n",
    "\n",
    "        softw2v2 = soft_w2v(postText, relatedPostText, model2, tfidf_vectorizer)\n",
    "        soft_train_SO.append(softw2v2)\n",
    "\n",
    "        softlev = soft_lev(postText, relatedPostText, model2, tfidf_vectorizer)\n",
    "        soft_train_lev.append(softlev)\n",
    "        cos_train_body.append(cos_similarity(postText, relatedPostText, model, tfidf_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(len(cos_train_body))\n",
    "# new_cos=[]\n",
    "# for i in cos_train_body:\n",
    "#     if type(i)==list:\n",
    "#         new_cos.append(i[0])\n",
    "#     else:\n",
    "#         new_cos.append(i)\n",
    "# print(len(new_cos))\n",
    "np_cos_train_body= np.array(cos_train_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n",
      "32000\n",
      "32000\n",
      "32000\n",
      "np_train_unigrams:  [24 13 33]\n",
      "32000\n",
      "32000\n",
      "32000\n",
      "all_feats:  [[1.05000000e+02 1.38000000e+02 1.99000000e+02 2.00000000e+00\n",
      "  5.00000000e+00 2.40000000e+01 4.79051750e-01 5.33337488e-01\n",
      "  3.80178128e-01 3.55610304e-01]\n",
      " [6.40000000e+01 9.40000000e+01 1.53000000e+02 0.00000000e+00\n",
      "  0.00000000e+00 1.30000000e+01 3.33382290e-01 3.76200833e-01\n",
      "  3.51270902e-01 2.18593091e-01]\n",
      " [1.36000000e+02 1.78000000e+02 2.90000000e+02 4.00000000e+00\n",
      "  1.60000000e+01 3.30000000e+01 3.77879165e-01 4.49246605e-01\n",
      "  3.45845517e-01 3.21577313e-01]]\n",
      "32000\n",
      "32000\n",
      "(32000, 10)\n"
     ]
    }
   ],
   "source": [
    "#train:\n",
    "print(len(train_q1_Body))\n",
    "\n",
    "np_train_char_five = np.array(train_char_five)\n",
    "print(len(np_train_char_five))\n",
    "np_train_char_four= np.array(train_char_four)\n",
    "np_train_char_three= np.array(train_char_three)\n",
    "print(len(train_char_three))\n",
    "np_train_trigrams= np.array(train_trigrams)\n",
    "\n",
    "np_train_bigrams = np.array(train_bigrams)\n",
    "print(len(np_train_bigrams))\n",
    "np_train_unigrams = np.array(train_unigrams)\n",
    "print(len(np_train_unigrams))\n",
    "print(\"np_train_unigrams: \", np_train_unigrams[:3])\n",
    "np_soft_train_google = np.array(soft_train_google)\n",
    "print(len(np_soft_train_google))\n",
    "np_soft_train_SO = np.array(soft_train_SO)\n",
    "print(len(np_soft_train_SO))\n",
    "np_soft_train_lev = np.array(soft_train_lev)\n",
    "print(len(soft_train_lev))\n",
    "\n",
    "\n",
    "train_all_feats = np.stack((np_train_char_five, np_train_char_four, np_train_char_three ,np_train_trigrams, np_train_bigrams, np_train_unigrams,\n",
    "                            np_soft_train_SO, np_soft_train_google, np_soft_train_lev, np_cos_train_body), axis=-1)\n",
    "print(\"all_feats: \", train_all_feats[:3])\n",
    "print(np_train_bigrams.size)\n",
    "print(np_train_unigrams.size)\n",
    "print(train_all_feats.shape)\n",
    "np.save(open('hand-feats/train_all_feats_noClue_medium.npy', 'wb'), train_all_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/anaconda3-5.2.0/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/share/apps/anaconda3-5.2.0/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "print(len(test_q1_Body))\n",
    "for i in range(len(test_q1_Body)):\n",
    "        \n",
    "    postText = test_q1_Body[i] \n",
    "    relatedPostText =  test_q2_Body[i]\n",
    "    \n",
    "    test_char_five.append(common_char_fivegrams(postText, relatedPostText, model))\n",
    "    test_char_four.append(common_char_fourgrams(postText, relatedPostText, model))\n",
    "    test_char_three.append(common_char_trigrams(postText, relatedPostText, model))\n",
    "    test_trigrams.append(common_trigrams(postText, relatedPostText, model))\n",
    "        \n",
    "    test_bigrams.append(common_bigrams(postText, relatedPostText, model))\n",
    "    test_unigrams.append(common_unigrams(postText, relatedPostText, model))\n",
    "    softw2v1 =soft_w2v(postText, relatedPostText, model, tfidf_vectorizer)\n",
    "\n",
    "    soft_test_google.append(softw2v1)\n",
    "\n",
    "    softw2v2=soft_w2v(postText, relatedPostText, model2, tfidf_vectorizer)\n",
    "\n",
    "    soft_test_SO.append(softw2v2)\n",
    "\n",
    "    softlev = soft_lev(postText, relatedPostText, model2, tfidf_vectorizer)\n",
    "\n",
    "    soft_test_lev.append(softlev)\n",
    "    if (len(test_char_five)!=len(soft_test_lev)):\n",
    "        print(i)\n",
    "    cos_test_body.append(cos_similarity(postText, relatedPostText, model, tfidf_vectorizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(cos_test_body))\n",
    "\n",
    "# new_cos=[]\n",
    "# for i in cos_test_body:\n",
    "#     if type(i)==list:\n",
    "#         new_cos.append(i[0])\n",
    "#     else:\n",
    "#         new_cos.append(i)\n",
    "# print(len(new_cos))\n",
    "np_cos_test_body= np.array(cos_test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n",
      "8000\n",
      "8000\n",
      "np_test_bigrams:  8000\n",
      "np_test_unigrams:  8000\n",
      "google:  8000\n",
      "so:  8000\n",
      "8000\n",
      "all_feats:  [[3.36000000e+02 4.30000000e+02 6.13000000e+02 1.10000000e+01\n",
      "  1.60000000e+01 4.60000000e+01 7.51451734e-01 7.71761344e-01\n",
      "  7.33846546e-01 6.78276176e-01]\n",
      " [4.10000000e+01 6.50000000e+01 1.44000000e+02 0.00000000e+00\n",
      "  0.00000000e+00 1.10000000e+01 1.37659936e-01 2.52587077e-01\n",
      "  8.58362485e-02 5.49876580e-02]\n",
      " [6.20000000e+01 1.42000000e+02 3.22000000e+02 0.00000000e+00\n",
      "  0.00000000e+00 1.90000000e+01 2.19694698e-01 3.48257934e-01\n",
      "  1.60015982e-01 6.29924873e-02]]\n",
      "8000\n",
      "8000\n",
      "(8000, 10)\n"
     ]
    }
   ],
   "source": [
    "#test:\n",
    "np_test_char_five = np.array(test_char_five)\n",
    "print(len(np_test_char_five))\n",
    "np_test_char_four= np.array(test_char_four)\n",
    "print(len(np_test_char_four))\n",
    "np_test_char_three= np.array(test_char_three)\n",
    "print(len(np_test_char_three))\n",
    "np_test_trigrams= np.array(test_trigrams)\n",
    "print(len(np_test_trigrams))\n",
    "np_test_bigrams = np.array(test_bigrams)\n",
    "print(\"np_test_bigrams: \", len(np_test_bigrams))\n",
    "np_test_unigrams = np.array(test_unigrams)\n",
    "print(\"np_test_unigrams: \", len(np_test_unigrams))\n",
    "np_soft_test_google = np.array(soft_test_google)\n",
    "print(\"google: \",len(np_soft_test_google))\n",
    "np_soft_test_SO = np.array(soft_test_SO)\n",
    "print(\"so: \",len(np_soft_test_SO))\n",
    "np_soft_test_lev = np.array(soft_test_lev)\n",
    "print(len(np_soft_test_lev))\n",
    "#np_cos_test_body= np.array(cos_test_body)\n",
    "# \n",
    "test_all_feats = np.stack((np_test_char_five,np_test_char_four,np_test_char_three,np_test_trigrams,np_test_bigrams, np_test_unigrams, np_soft_test_SO, np_soft_test_google, np_soft_test_lev, np_cos_test_body), axis=-1)\n",
    "print(\"all_feats: \", test_all_feats[:3])\n",
    "print(np_test_bigrams.size)\n",
    "print(np_test_unigrams.size)\n",
    "print(test_all_feats.shape)\n",
    "np.save(open('hand-feats/test_all_feats_noClue_medium.npy', 'wb'), test_all_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
